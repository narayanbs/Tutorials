Everything is bits in computers, (0 and 1). The text that we see on our display (characters, numbers, symbols) and understand,
are all bits or bytes that are interpreted in a certain way. The way it is interpreted is dependent on a standard known as 
character encoding. "Character encoding" is the process of converting characters into their byte mappings according to a specific
character set. Using character encoding, computers can digitally translate characters from a single language into others.
The applications store the encoded characters either in bits or bytes.

All characters are encoded before being displayed on a terminal, written to a disk, printed on paper, or transmitted through a digital medium

You edit a file and save it. It stores information in a particular encoding. Applications that use the file will need to know the encoding
to retrieve the information they seek. 
for ex: For the vim editor we can set the encoding in .vimrc as shown below
set fileencoding=utf-8    // the output encoding of the file that is written.

Similarly terminals use their own encoding to display text. 
for ex: for the vim editor we can set the output encoding as shown below
set encoding=utf-8    

If encoding is not specified explicitly then programs usually get it from the locale through the OS. 

Each program will have a default encoding, that decides how the characters are represented internally. Most programs default to UTF-8 these days.
Vim does not actually default 'encoding' to UTF-8. It defaults to latin1, but will change based on the locale of your environment.
It's a good idea to always put set encoding=utf-8 in your vimrc. It automatically enables saner encoding detection settings for 'fileencodings' and supports any characters that you'll need to store.

Neovim decided to only support encoding=utf-8. This vastly simplifies the code, since now everything is working with UTF-8 internally, 
and only has to encode/decode when interfacing with the outside world (saving files in specific encodings, 
converting input from the locale's encoding, etc.).

When it comes to text, there are two things that is done
* Encoding: mapping characters to numbers.. this is where standards such as unicode, iso8859... come to picture
* Display: Displaying these numbers as characters on the display hardware. These require fonts that has the character and render it. 
There are default fonts in OS or user provided ones that the OS must use to map numbers to characters and render it.

-----------------------------------------------------
How does font rendering actually work??

In general, applications such as terminals use libraries provided by the OS or a GUI toolkit to do font rendering.
Typical font engines allow a number of modes of operation. For the simple case, an application can ask for a string of text to be drawn 
at a certain position, and the engine takes care of everything (measurement, positioning, drawing the pixels to the display, etc).

For applications which require a finer degree of control - browsers or word processors, for example - the engine will expose interfaces 
where the app can ask for a given piece of text to be measured in advance. 
The app can then use this knowledge to work out how much text it can fit on a line, where the line-breaks should be,
how much room a paragraph will take, etc. The app can still ask the engine to do the actual rendering of the pixels.

(There may be an in-between scenario where the engine can take a maximum-width parameter, and possibly some kerning/padding parameters, 
and automatically render as much text as it can fit.)

Finally, the font engine might allow the app to take over the final rendering of the text, by returning bitmaps of glyphs pre-rendered 
at a certain size, allowing the app to position and composite it onto the final display. 
Or the engine might even offer to return the raw glyph outline data for rendering with some vector toolkit.

-------------------------------------------------------
Locale
---------
A locale comprises the language, territory, and code set combination used to identify a set of language conventions. 
These conventions include information on collation, case conversion, and character classification, the language of message catalogs, 
date-and-time representation, the monetary symbol, numeric representation, currency format, paper size and color setting.

Locale information is contained in locale definition source files that are converted into a locale database and stored in OS. The 
locale definition source files are named in the folowing format: language[_territory][.codeset][@modifier]
for ex: en_US.UTF-8

To deal with locale data in a logical manner, locale definition source files are divided into six categories. 
Each category contains a specific aspect of the locale data. 

Understanding Locale categories
-------------------------------
LC_COLLATE -- Defines character-collation or string-collation information.
LC_CTYPE -- Defines character classification, case conversion, and other character attributes.
LC_MESSAGES -- Defines the format for affirmative and negative responses.
LC_MONETARY -- Defines rules and symbols for formatting monetary numeric information.
LC_NUMERIC -- Defines rules and symbols for formatting nonmonetary numeric information.
LC_TIME -- Defines a list of rules and symbols for formatting time and date information.

GNU libc, which you'll find on non-embedded Linux, defines additional locale categories:

LC_PAPER: the default paper size (defined by height and width).
LC_NAME, LC_ADDRESS, LC_TELEPHONE, LC_MEASUREMENT, LC_IDENTIFICATION: I don't know of any application that uses these

Environment variables
----------------------
Applications that use locale settings determine them from environment variables, like the LANG environment variable and
LC_* environment variables.

LANG - specifies the installation default locale. This is the locale every process uses unless LC_* environment variables are explicity set.
  if the LANG envionment variable is not set, it defaults to C

LC_ALL - Overrides the value of the LANG environment variable and the values of any other LC_* environment variables.
  One important setting is to set LC_ALL to "". for ex: locale.setlocale(locale.LC_ALL, "") This sets the locale for all
  categories to the default setting.(typically specified in the LANG environment variable)

LC_* - Categories specified above can be used as environment variables. 

The environment variables that affect locale selection can be grouped into three priority classes: high, medium, and low. 
Environment variables in the high priority class are:
	LC_ALL
	LC_COLLATE
	LC_CTYPE
Environment variables in the medium priority class are:
	LC_MESSAGES
	LC_MONETARY
	LC_NUMERIC
	LC_TIME
The environment variable in the low priority class is:
	LANG

Along with this the unix systems also contain a C and Posix locale, that defines the ANSI C-defined standard locale inherited by all
processes at startup time.

-------------

On ubuntu, use locale command to get the current locale settings. 

$ locale

LANG=en_US.UTF-8
LANGUAGE=en_US.UTF-8:en
LC_CTYPE="en_US.UTF-8"
LC_NUMERIC="en_US.UTF-8"
LC_TIME="en_US.UTF-8"
LC_COLLATE="en_US.UTF-8"
LC_MONETARY="en_US.UTF-8"
LC_MESSAGES="en_US.UTF-8"
LC_PAPER="en_US.UTF-8"
LC_NAME="en_US.UTF-8"
LC_ADDRESS="en_US.UTF-8"
LC_TELEPHONE="en_US.UTF-8"
LC_MEASUREMENT="en_US.UTF-8"
LC_IDENTIFICATION="en_US.UTF-8"
LC_ALL=


$ locale -a to get all the available locales.

C
C.utf8
en_AG
en_AG.utf8
en_AU.utf8
en_BW.utf8
en_CA.utf8
en_DK.utf8
en_GB.utf8
en_HK.utf8
en_IE.utf8
en_IL
en_IL.utf8
en_IN
en_IN.utf8
en_NG
en_NG.utf8
en_NZ.utf8
en_PH.utf8
en_SG.utf8
en_US.utf8
en_ZA.utf8
en_ZM
en_ZM.utf8
en_ZW.utf8
POSIX

$ locale -m to get the available charmaps (character set description files), available in the OS. These files describe how
locales and agents are mapped to character encodings.

$ locale -m 

NS_4551-1
NS_4551-2
PT
PT154
PT2
RK1048
SAMI
SAMI-WS2
SEN_850200_B
SEN_850200_C
SHIFT_JIS
SHIFT_JISX0213
T.101-G2
T.61-7BIT
T.61-8BIT
TCVN5712-1
TIS-620
TSCII
UTF-8
VIDEOTEX-SUPPL
VISCII
WIN-SAMI-2
WINDOWS-31J

$ locale -c to get information for a specific category. 

$ locale -c LC_TIME 
LC_TIME
Sun;Mon;Tue;Wed;Thu;Fri;Sat
Sunday;Monday;Tuesday;Wednesday;Thursday;Friday;Saturday
Jan;Feb;Mar;Apr;May;Jun;Jul;Aug;Sep;Oct;Nov;Dec
  ...
  ...
  ...

$ locale -c charmap
LC_CTYPE
UTF-8

Note:
The LC_ALL variable sets all locale variables output by the command 'locale -a'.  It is a convenient way of specifying a language environment 
with one variable, without having to specify each LC_* variable. 
for ex:
	$ LC_TIME=fr_FR.UTF-8 date
	jeudi 22 ao√ªt 2013, 10:41:30 (UTC+0100)
		Or:
	$ LC_MONETARY=fr_FR.UTF-8 locale currency_symbol
	‚Ç¨
		Or override everything with LC_ALL.
	$ LC_ALL=es_ES man
	¬øQu√© p√°gina de manual desea?

	$ LC_ALL=C man
	What manual page do you want?

$ LC_ALL=C locale -c charmap
LC_CTYPE
ANSI_X3.4-1968

Another example: forcing sorting bytewise

	$ LC_ALL=en_US sort <<< $'a\nb\nA\nB'
	a
	A
	b
	B

	$ LC_ALL=C sort <<< $'a\nb\nA\nB'
	A
	B
	a
	b

Recommendation
-------------------
The useful settings are:

1. Set LC_CTYPE to the language and encoding that you encode your text files in. Ensure that your terminals use that encoding.
  For most languages, only the encoding matters. There are a few exceptions; for example, an uppercase i is I in most languages but ƒ∞ in Turkish (tr_TR).
2. Set LC_MESSAGES to the language that you want to see messages in.
3. Set LC_PAPER to en_US if you want US Letter to be the default paper size and just about anything else (e.g. en_GB) if you want A4.
4. Set LC_TIME to your favorite time format.
5. Avoid setting LC_COLLATE and LC_NUMERIC. If you use LANG, explicitly override these two categories by setting them to C.

Read Articles/What_should_my_locale_be.html  for more details

----------------------------------------------------------------------------------------------
--------------------------------------------------------------------
Character encoding is basically set of characters encoded into numbers. 

ISO 646 -- Ascii taking values -- 0 to 127 
ISO 8859-x -- 0 to 127 is the same ascii, 127 to 255 is different
for ex: ISO 8859-1 is Latin-1, ISO 8859-2 is Latin-2.. 

There are various other encodings that use a byte of values, with the base ascii and others after for ex: windows-1252 or cp-1252

ISO 10646 -- Unicode. 

Unicode specifies code points, with each code point (hexadecimal numbers) indicating a character. Initially the code points were 16-bit and could represent 65536
characters, but now the number of characters has extended to 21-bit. The initial set of 16-bit encodings is known as Basic Multilingual Plane (BMP).

Fixed-length UCS-2 (16-bit i.e 2 bytes) was used to represent these code points. UCS-2 is redundant now since it can only represent the BMP.
UCS-4 is another fixed-length encoding that uses 4 bytes to represent the code points. 

Fixed length encodings waste space, so (multi-byte) variable length encodings were used, These include utf-8, utf-16 and utf-32.

-----------------------------------------------
UTF-8 is a variable-width character encoding scheme used to encode text that supports a wide range of characters 
beyond the traditional ASCII set. 

In UTF-8:
- Characters are represented using one to four bytes.
- The first byte of each character is a marker that indicates how many bytes follow to complete the character's encoding.

Here's how multibyte sequences are identified in UTF-8:

1. **First Byte Identification**:
   - The first byte of a UTF-8 sequence determines how many bytes the character occupies and its general structure.
   - It serves as a marker that indicates whether the character is a single-byte character (ASCII compatible) or part of a multibyte sequence.
   - The structure of the first byte helps in determining the number of bytes used for encoding:
     - If the first byte starts with `0xxxxxxx`, it represents a single-byte ASCII character (0-127).
     - If the first byte starts with `110xxxxx`, it indicates a two-byte sequence.
     - If the first byte starts with `1110xxxx`, it indicates a three-byte sequence.
     - If the first byte starts with `11110xxx`, it indicates a four-byte sequence.
     - Bytes starting with `10xxxxxx` are continuation bytes used in multibyte sequences (following bytes).

2. **Subsequent Bytes (Continuation Bytes)**:
   - Following bytes in a multibyte sequence all start with the bit pattern `10xxxxxx`.
   - These bytes indicate that they are part of a multibyte sequence and provide the additional bits needed to complete the character's encoding.

3. **Identifying Complete Characters**:
   - By examining the first byte, you can determine how many bytes (and which bytes) belong to a particular character.
   - The subsequent bytes (continuation bytes) validate that they follow the UTF-8 rules, ensuring they are not misinterpreted as standalone characters.

### Example:
Let's take an example of UTF-8 encoding for the character 'Œª' (lambda), which is U+03BB in Unicode.

- In binary, U+03BB is represented as `00000011 10111011`.

- UTF-8 encoding:
  - U+03BB requires two bytes in UTF-8: `11000011 10111011`.
  - The first byte `11000011` indicates it's a two-byte sequence (`110` prefix).
  - The second byte `10111011` starts with `10` as a continuation byte.

By examining the first byte (`11000011`), a program or parser can correctly identify that this sequence represents the character 'Œª' 
in UTF-8 encoding and decode it accordingly.

In summary, multibyte UTF-8 sequences are identified based on the structure of their initial byte, which indicates both 
the number of bytes in the sequence and the structure that subsequent bytes must follow. 
This structured approach ensures accurate encoding and decoding of characters across different platforms and systems that support UTF-8.

UTF-16 is another character encoding scheme that stands for "Unicode Transformation Format - 16-bit". 
It encodes each Unicode code point as either one or two 16-bit code units. Here's how UTF-16 is recognized and interpreted:

1. **Byte Order Mark (BOM)**:
   - UTF-16 uses a Byte Order Mark (BOM) at the beginning of the encoded data to indicate the byte order (endianness) of the text. 
   - The BOM is a specific sequence of bytes (`FE FF` for big-endian, `FF FE` for little-endian) that serves as an indicator 
   for applications to determine how the subsequent 16-bit code units are arranged in memory.
   - For example:
     - `FE FF` indicates big-endian UTF-16.
     - `FF FE` indicates little-endian UTF-16.
   - The BOM helps in correctly interpreting the sequence of 16-bit code units that follow it.

2. **Interpreting 16-bit Code Units**:
   - Once the BOM (if present) indicates the byte order, UTF-16 text is then interpreted by parsing 16-bit code units.
   - Each Unicode code point is encoded using one or two 16-bit code units (also known as code points in the UTF-16 scheme).
   - Single 16-bit code units represent characters from the Basic Multilingual Plane (BMP), which includes most commonly 
   used characters and symbols.
   - If a character lies outside the BMP (e.g., Supplementary Planes), it is represented by a surrogate pair consisting
of two 16-bit code units (one from the high-surrogates range and one from the low-surrogates range).

3. **Handling Surrogate Pairs**:
   - Surrogate pairs are pairs of 16-bit code units that together represent a single Unicode code point beyond the BMP.
   - High-surrogate code units (`D800` to `DBFF`) and low-surrogate code units (`DC00` to `DFFF`) are used together to encode supplementary characters.
   - Applications need to recognize surrogate pairs and correctly interpret them to reconstruct the actual Unicode code point.

### Example:
Let's take an example 

When encoded in UTF-16 with the BOM FE FF, the character 'Œª' (U+03BB) will be represented as the sequence of bytes: FE FF 03 3B.
Here's how it breaks down:
* FE FF: Byte Order Mark indicating UTF-16 big-endian encoding.
* 03 3B: The 16-bit code unit representing the character 'Œª'.

When encoded in UTF-16 with littleendian encoding using the BOM FF FE, the character 'Œª' (U+03BB) will be represented as 
the sequence of bytes: FF FE 3B 03.
Here's how it breaks down:
* FF FE: Byte Order Mark indicating UTF-16 little-endian encoding.
* 3B 03: The 16-bit code unit representing the character 'Œª'.

UTF-16 encoded text without a BOM (assuming little-endian by default):
- The character 'Œª' (U+03BB) in UTF-16 would be represented as `03BB` (in hexadecimal).
- If the text starts with `03 BB`, an application interpreting UTF-16 would:
  - Recognize the sequence `03 BB` as representing the character 'Œª'.
  - Since there is no BOM, it assumes little-endian encoding by default.
  
  
For characters requiring surrogate pairs (e.g., U+1F600, which is üòÄ), the UTF-16 encoding would use two 16-bit code units: `D83D DE00`.

In summary, UTF-16 encoding is recognized primarily through the presence of a Byte Order Mark (BOM) at the beginning of the data, 
which indicates the byte order of the following 16-bit code units. Applications then interpret these units, handling single code units 
for characters within the Basic Multilingual Plane (BMP) and surrogate pairs for characters outside the BMP to correctly represent 
and display Unicode text.

----------------------------------------------------------------------------------------

C specification basically specifies a basic character set and an extended character set. The extended character set is implementation defined.
* All characters in the basic character set should be represented in a byte
* All bits in the null character '\0' should be zero.
* The characters representing the digits 0 - 9 should have values that are obtained by incrementing the previous number by 1. 

Multibyte characters 
--------------------
When someone says "C string", you know that they are referring to an array of multi-byte characters that are terminated by null character '\0'.
for ex:
    char s[128] = "Hello World";

What we‚Äôre saying here is that a particular character that‚Äôs not in the basic character set could be composed of multiple bytes. 
Up to MB_LEN_MAX of them (from <limits.h>). Sure, it only looks like one character on the screen, but it could be multiple bytes.
The MB_LEN_MAX macro is the maximum number of bytes needed to represent a single wide character, in any of the supported locales.
In glibc, MB_LEN_MAX is typically 16, while sizeof(wchar_t) is 4.

You can throw Unicode values in there, as well, as we saw earlier:

char *s = "\u20AC1.23";
printf("%s\n", s);  // ‚Ç¨1.23

But here we‚Äôre getting into some weirdness, because check this out:

char *s = "\u20AC1.23";  // ‚Ç¨1.23
printf("%zu\n", strlen(s));  // 7

The string length of "‚Ç¨1.23" is 7?! Yes! Well, on my system, yes! Remember that strlen() returns the number of bytes in the string, not the 
number of characters. 

How about char constants?????
while C allows individual multibyte char constants, the behavior of these varies by implementation and your compiler might warn on it.
GCC, for example, warns of multi-character character constants for the following two lines (and, on my system, prints out the UTF-8 encoding):

printf("%x\n", '‚Ç¨');
printf("%x\n", '\u20ac');

Wide Characters
-----------------------------
A wide character is a single value that can uniquely represent any character in the current locale. 
Basically, where multibyte character strings are arrays of bytes, wide character strings are arrays of characters. So you can start thinking 
on a character-by-character basis rather than a byte-by-byte basis

Wide characters can be represented by a number of types, but the big standout one is wchar_t. It‚Äôs like char, except wide.
It could be 16 bits or it could be 32 bits. To use wchar_t, include <wchar.h>. 

But wait, you‚Äôre saying‚Äîif it‚Äôs only 16 bits, it‚Äôs not big enough to hold all the Unicode code points, is it? You‚Äôre right‚Äîit‚Äôs not. 
The spec doesn‚Äôt require it to be. It just has to be able to represent all the characters in the current locale.
This can cause grief with Unicode on platforms with 16-bit wchar_ts (ahem‚ÄîWindows).

You can declare a string or character of this type with the L prefix, and you can print them with the %ls (‚Äúell ess‚Äù) format specifier. 
Or print an individual wchar_t with %lc.

wchar_t *s = L"Hello, world!";
wchar_t c = L'B'; // or wchar_t c = L'\u20AC';
printf("%ls %lc\n", s, c);

Now‚Äîare those characters stored as Unicode code points, or not? Depends on the implementation. But you can test if they are with the 
macro __STDC_ISO_10646__. If this is defined, the answer is, ‚ÄúIt‚Äôs Unicode!‚Äù

Multibyte to wchar_t Conversions
----------------------------------
So how do we get from the byte-oriented standard strings to the character-oriented wide strings and back?

We can use a couple string conversion functions to make this happen.

First, some naming conventions you‚Äôll see in these functions:

* mb: multibyte
* wc: wide character
* mbs: multibyte string
* wcs: wide character string

Conversion Function	Description
-------------------------------
mbtowc()	Convert a multibyte character to a wide character.
wctomb()	Convert a wide character to a multibyte character.
mbstowcs()	Convert a multibyte string to a wide string.
wcstombs()	Convert a wide string to a multibyte string.

ex:

#include <stdio.h>
#include <stdlib.h>
#include <wchar.h>
#include <string.h>
#include <locale.h>

int main(void)
{
    // Get out of the C locale to one that likely has the euro symbol
    // If locale is an empty string, "", each part of the locale that should be modified is set according to the environment variables
    setlocale(LC_ALL, "");

    // Original multibyte string with a euro symbol (Unicode point 20ac)
    char *mb_string = "The cost is \u20ac1.23";  // ‚Ç¨1.23
    size_t mb_len = strlen(mb_string);

    // Wide character array that will hold the converted string
    wchar_t wc_string[128];  // Holds up to 128 wide characters

    // Convert the MB string to WC; this returns the number of wide chars
    size_t wc_len = mbstowcs(wc_string, mb_string, 128);

    // Print result--note the %ls for wide char strings
    printf("multibyte: \"%s\" (%zu bytes)\n", mb_string, mb_len);
    printf("wide char: \"%ls\" (%zu characters)\n", wc_string, wc_len);
    printf("Length of wide string: %ls is %zu\n", wc_string, wcslen(wc_string));
}

Output:
multibyte: "The cost is ‚Ç¨1.23" (19 bytes)
wide char: "The cost is ‚Ç¨1.23" (17 characters)

One interesting thing to note is that mbstowcs(), in addition to converting the multibyte string to wide, returns the length (in characters) 
of the wide character string. On POSIX-compliant systems, you can take advantage of a special mode where it only returns the length-in-characters
of a given multibyte string: you just pass NULL to the destination, and 0 to the maximum number of characters to convert (this value is ignored).

(In the code below, I‚Äôm using my extended source character set‚Äîyou might have to replace those with \u escapes.)

setlocale(LC_ALL, "");

// The following string has 7 characters
size_t len_in_chars = mbstowcs(NULL, "¬ß¬∂¬∞¬±œÄ‚Ç¨‚Ä¢", 0);

printf("%zu", len_in_chars);  // 7

Again, that‚Äôs a non-portable POSIX extension.
And, of course, if you want to convert the other way, it‚Äôs wcstombs().



